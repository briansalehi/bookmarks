# Mastering Linux Device Driver Development
<img src="../covers/9781789342208.jpg" width="200"/>

## Chapter 1/14

<details>
<summary>How many synchronization mechanisms for accessibility of shared resources are available in the kernel?</summary>

> We can enumerate two synchronization mechanisms, as follows:
>
> 1. **Locks**: Used for mutual exclusion. When one contender holds the lock, no other can hold it (others are excluded). The most known locks in the kernel are **spinlocks** and **mutexes**.
>
> A resource is said to be shared when it is accessible by several contenders, whether exclusively or not.
> When it is exclusive, access must be synchronized so that only the allowed contender(s) may own the resource.
>
> The operating system performs mutual exclusion by atomically modifying a variable that holds the current state of the resource, making this visible to all contenders that might access the variable at the same time.
>
> 2. **Conditional variables**: For waiting for a change. These are implemented differently in the kernel as **wait queues** and **completion queues**.
>
> Apart from dealing with the exclusive ownership of a given shared resource, there are situations where it is better to wait for the state of the resource to change.
>
> The Linux kernel does not implement conditional variables, but to achieve the same or even better, the kernel provides the following mechanisms:
>
> * **Wait queue**: To wait for a change â€” designed to work in concert with locks.
> * **Completion queue**: To wait for the completion of a given computation, mostly used with DMAs.

> Origin: 1

> References:
---
</details>

<details>
<summary>What is a spinlock?</summary>

> A *spinlock* is a hardware-based locking primitive that depends on hardware capabilities to provide atomic operations (such as `test_and_set`, which in a non-atomic implementation would result in read, modify, and write operations).
> It is the simplest and the base locking primitive.
>
> When *CPUB* is running, and task B wants to acquire the spinlock while *CPUA* has already called this spinlock's locking function, *CPUB* will simply spin around a `while` loop until the other CPU releases the lock.
>
> This spinning will only happen on multi-core machines because, on a single-core machine, it cannot happen.
>
> A *spinlock* is said to be a lock held by a CPU, in contrast to a *mutex* which is a lock held by a task.

> Origin: 1

> References:
---
</details>

<details>
<summary>How does spinlocks operate on a CPU?</summary>

> A spinlock operates by disabling the scheduler on the local CPU.
>
> This also means that a task currently running on that CPU cannot be preempted except by **interrupt requests (IRQs)** if they are not disabled on the local CPU.
> In other words, spinlocks protect resources that only one CPU can take/access at a time.
>
> This makes spinlocks suitable for **symmetrical multiprocessing (SMP)** safety and for executing atomic tasks.

> Origin: 1

> References:
---
</details>

<details>
<summary>Define a spinlock in module source?</summary>

> A spinlock is created either statically using a `DEFINE_SPINLOCK` macro:
>
> ```c
> static DEFINE_SPINLOCK(my_spinlock);
> ``````
>
> This macro is defined in `include/linux/spinlock_types.h`.
>
> For dynamic (runtime) allocation, it's better to embed the spinlock into a bigger structure, allocating memory for this structure and then calling `spin_lock_init()` on the spinlock element:
>
> ```c
> struct bigger_struct {
>     spinlock_t lock;
>     unsigned int foo;
>     [...]
> };
> static struct bigger_struct *fake_init_function()
> {
>     struct bigger_struct *bs;
>     bs = kmalloc(sizeof(struct bigger_struct), GFP_KERNEL);
>     if (!bs)
>         return -ENOMEM;
>     spin_lock_init(&bs->lock);
>     return bs;
> }
> ``````

> Origin: 1

> References:
---
</details>

<details>
<summary>Lock a previously defined spinlock in module source?</summary>

> We can lock/unlock the spinlock using `spin_lock()` and `spin_unlock()` inline functions, both defined in `include/linux/spinlock.h`:
>
> ```c
> static __always_inline void spin_unlock(spinlock_t *lock);
> static __always_inline void spin_lock(spinlock_t *lock);
> ``````

> Origin: 1

> References:
---
</details>

<details>
<summary>What are the limitations of locking/unlocking spinlocks in a kernel module?</summary>

> Though a spinlock prevents preemption on the local CPU, it does not prevent this CPU from being hogged by an interrupt.
>
> Imagine a situation where the CPU holds a *"spinlock"* on behalf of task A in order to protect a given resource, and an interrupt occurs.
> The CPU will stop its current task and branch to this interrupt handler.
> Now, imagine if this IRQ handler needs to acquire this same spinlock.
> It will infinitely spin in place, trying to acquire a lock already locked by a task that it has preempted which results in a deadlock.

> Origin: 1

> References:
---
</details>

<details>
<summary>Prevent deadlock caused by IRQs when using spinlocks?</summary>

> To address this issue, the Linux kernel provides `_irq` variant functions for spinlocks, which, in addition to disabling/enabling preemption, also disable/enable interrupts on the local CPU.
> These functions are `spin_lock_irq()` and `spin_unlock_irq()`, defined as follows:
>
> ```c
> static void spin_unlock_irq(spinlock_t *lock)
> static void spin_lock_irq(spinlock_t *lock)
> ``````

> Origin: 1

> References:
---
</details>

<details>
<summary>How does spinlocks affect preemtion after locking and unlocking?</summary>

> `spin_lock()` and all its variants automatically call `preempt_disable()`, which disables preemption on the local CPU, while `spin_unlock()` and its variants call `preempt_enable()`, which tries to enable preemption, and which internally calls schedule() if enabled.
> `spin_unlock()` is then a preemption point and might re-enable preemption.

> Origin: 1

> References:
---
</details>

<details>
<summary>Store and restore previous IRQs status when using spinlocks?</summary>

> `spin_lock_irq()` function is unsafe when called from IRQs off-context as its counterpart `spin_unlock_irq()` will dumbly enable IRQs, with the risk of enabling those that were not enabled while `spin_lock_irq()` was invoked.
> It makes sense to use `spin_lock_irq()` only when you know that interrupts are enabled.
>
> To achieve this, the kernel provides `_irqsave` variant functions that behave exactly like the `_irq` ones, with saving and restoring interrupts status features in addition.
> These are `spin_lock_irqsave()` and `spin_lock_irqrestore()`, defined as follows:
>
> ```c
> spin_lock_irqsave(spinlock_t *lock, unsigned long flags)
> spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
> ``````
>
> `spin_lock()` and all its variants automatically call `preempt_disable()`, which disables preemption on the local CPU, while `spin_unlock()` and its variants call `preempt_enable()`, which tries to enable preemption, and which internally calls `schedule()` if enabled depending on the current value of the counter, whose current value should be 0.</br>
> It tries because it depends on whether other spinlocks are locked, which would affect the value of the preemption counter.
> `spin_unlock()` is then a preemption point and might re-enable preemption.

> Origin: 1

> References:
---
</details>

<details>
<summary>How a critical section can be protected from being preemted by kernel?</summary>

> Though disabling interrupts may prevent kernel preemption nothing prevents the protected section from invoking the `schedule()` function.
> The kernel disables or enables the scheduler, and thus preemtion, by increasing or decreasing a kernel global and per-CPU variable called `preempt_count` with 0 as default value.
> This variable is checked by the `schedule()` function and when it is greater than 0, the scheduler simply returns and does nothing.
> This variable is incremented at each invocation of a `spin_lock*()` family function.
> On the other side, releasing a spinlock decrements it from 1, and whenever it reaches 0, the scheduler is invoked, meaning that your critical section would not be that atomic.
>
> Thus, disabling interrupts protects you from kernel preemption only in cases where the protected code does not trigger preemption itself.
> That said, code that locked a spinlock may not sleep as there would be no way to wake it up as timer interrupts and/or schedulers are disabled on the local CPU.

> Origin: 1

> References:
---
</details>

<details>
<summary>What is a mutex and how does it operate?</summary>

> It behaves exactly like a *spinlock*, with the only difference being that your code can sleep.
> A spinlock is a lock held by a CPU, a mutex, on the other hand, is a lock held by a task.
>
> A mutex is a simple data structure that embeds a wait queue to put contenders to sleep and a spinlock to protect access to this wait queue.
>
> ```c
> struct mutex {
>     atomic_long_t owner;
>     spinlock_t wait_lock;
> #ifdef CONFIG_MUTEX_SPIN_ON_OWNER
>     struct optimistic_spin_queue osq; /* Spinner MCS lock */
> #endif
>     struct list_head wait_list;
>     [...]
> };
> ``````
>
> The mutex APIs can be found in the `include/linux/mutex.h` header file.

> Origin: 1

> References:
---
</details>

<details>
<summary>Initialize a mutex in the kernel?</summary>

> As for other kernel core data structures, there is a static initialization:
>
> ```c
> static DEFINE_MUTEX(my_mutex);
> ``````
>
> A second approach the kernel offers is dynamic initialization, possible thanks to a call to a `__mutex_init()` low-level function, which is actually wrapped by a much more user-friendly macro, `mutex_init()`.
>
> ```c
> struct fake_data {
>     struct i2c_client *client;
>     u16 reg_conf;
>     struct mutex mutex;
> };
>
> static int fake_probe(struct i2c_client *client)
> {
>     [...]
>         mutex_init(&data->mutex);
>     [...]
> }
> ``````

> Origin: 1

> References:
---
</details>

<details>
<summary>Acquire a mutex in the kernel?</summary>

> Acquiring (aka locking) a mutex is as simple as calling one of the following three functions:
>
> ```c
> void mutex_lock(struct mutex *lock);
> int mutex_lock_interruptible(struct mutex *lock);
> int mutex_lock_killable(struct mutex *lock);
> ``````
>
> With `mutex_lock()`, your task will be put in an uninterruptible sleep state (`TASK_UNINTERRUPTIBLE`) while waiting for the mutex to be released if it is held by another task.
>
> `mutex_lock_interruptible()` will put your task in an interruptible sleep state, in which the sleep can be interrupted by any signal.
>
> `mutex_lock_killable()` will allow your sleeping task to be interrupted only by signals that actually kill the task.
>
> Each of these functions returns 0 if the lock has been acquired successfully.
> Moreover, interruptible variants return `-EINTR` when the locking attempt was interrupted by a signal.

> Origin: 1

> References:
---
</details>

<details>
<summary>Release an acquired mutex in the kernel?</summary>

> Whichever locking function is used, only the mutex owner should release the mutex using `mutex_unlock()`:
>
> ```c
> void mutex_unlock(struct mutex *lock);
> ``````

> Origin: 1

> References:
---
</details>

<details>
<summary>Check mutex locking availability before acquiring it?</summary>

> ```c
> static bool mutex_is_locked(struct mutex *lock);
> ``````
>
> This function simply checks if the mutex owner is `NULL` and returns `true` if so or `false` otherwise.

> Origin: 1

> References:
---
</details>

<details>
<summary>What are specific rules while using mutexes in the kernel?</summary>

> The most important ones are enumerated in the `include/linux/mutex.h` kernel mutex API header file, and some of these are outlined here:
>
> * A mutex can be held by one and only one task at a time.
> * Once held, the mutex can only be unlocked by the owner which is the task that locked it.
> * Multiple, recursive, or nested locks/unlocks are not allowed.
> * A mutex object must be initialized via the API. It must not be initialized by copying nor by using `memset()`, just as held mutexes must not be reinitialized.
> * A task that holds a mutex may not exit, just as memory areas where held locks reside must not be freed.
> * Mutexes may not be used in hardware or software interrupt contexts such as tasklets and timers.
>
> All this makes mutexes suitable for the following cases:
>
> * Locking only in the user context.
> * If the protected resource is not accessed from an IRQ handler and the operations need not be atomic.

> Origin: 1

> References:
---
</details>

<details>
<summary>What is more efficient between spinlocks and mutexes compared in terms of CPU cycles?</summary>

> It may be cheaper to use spinlocks for very small critical sections since the spinlock only suspends the scheduler and starts spinning, compared to the cost of using a mutex, which needs to suspend the current task and insert it into the mutex's wait queue, requiring the scheduler to switch to another task and rescheduling the sleeping task once the mutex is released.

> Origin: 1

> References:
---
</details>

<details>
<summary>Acquire a lock only if it is not already held by another contender?</summary>

> Such methods try to acquire the lock and immediately return a status value, showing whether the lock has been successfully locked or not.
>
> Both spinlock and mutex APIs provide a trylock method.
> These are, respectively, `spin_trylock()` and `mutex_trylock()`
>
> Both methods return 0 on failure (the lock is already locked) or 1 on success (lock acquired).
> Thus, it makes sense to use these functions along with an if statement:
>
> ```c
> int mutex_trylock(struct mutex *lock)
> ``````
>
> `spin_trylock()` will lock the spinlock if it is not already locked, just as the `spin_lock()` method does.
> However, it immediately returns 0 without spinning in cases where the spinlock is already locked:
>
> ```c
> static DEFINE_SPINLOCK(foo_lock);
>
> static void foo(void)
> {
>     if (!spin_trylock(&foo_lock)) {
>         /* Failure! the spinlock is already locked */
>         return;
>     }
>
>     /*
>      * reaching this part of the code means that the
>      * spinlock has been successfully locked
>      */
>     spin_unlock(&foo_lock);
> }
> ``````

> Origin: 1

> References:
---
</details>

## Chapter 2/14
## Chapter 3/14
## Chapter 4/14
## Chapter 5/14
## Chapter 6/14
## Chapter 7/14
## Chapter 8/14
## Chapter 9/14
## Chapter 10/14
## Chapter 11/14
## Chapter 12/14
## Chapter 13/14
## Chapter 14/14
