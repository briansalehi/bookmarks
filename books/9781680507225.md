# [A Common-Sense Guide to Data Structures and Algorithms](#)
<img alt="A Common-Sense Guide to Data Structures and Algorithms" src="../covers/9781680507225.jpg" width="200"/>

## Chapter 1/20

<details>
<summary>What are the main data structure operations?</summary>

> - Read
> - Search
> - Insert
> - Delete

> Origin: 1

> References:
---
</details>

<details>
<summary>How do we measure the speed of an operation in code?</summary>

> We can measure the speed of an operation in terms of how many computational steps it takes.

> Origin: 1

> References:
---
</details>

<details>
<summary>How many steps does each of four main operations take for an array?</summary>

> **Read:** Computers read an array in just one step.
>
> **Search:** To Search for a value within an array, computers have no choice but to inspect each cell one at a time.
> This algorithm is called **linear search**.
>
> **Insert:** Inserting data in an array can take N+1 steps for an array containing N elements.
> This is because in the worst case scenario we need to shift all N elements over, and then finally execute the insertion step.
>
> **Delete:** For an array containing N elements, the maximum number of steps that deletion would take is N steps.
> This is because we need one deletion and N-1 shifts.

> Origin: 1

> References:
---
</details>

<details>
<summary>What's the difference between an array-based set and an array?</summary>

> The only difference between array-based set and an array is that the set never allows duplicate values to be inserted into it.

> Origin: 1

> References:
---
</details>

<details>
<summary>How many steps does each of four main operations take for an array-based set?</summary>

> **Reading:** Reading from an array-based set is exactly as reading from an array, it takes just one step for a computer to look up what's contained within a particular index.
>
> **Search:** Searching an array-based set also turns out to be no different than searching an array, it takes up to N steps to search for a value within an array-based set.
>
> **Delete:** Deletion is also identical between an array-based set and an array.
> In the worst case scenario, it takes N steps to delete a value within an array-based set.
>
> **Insert:** Insertion, however is different between arrays and array-based set.
> With an array it takes N shifts and one insertion step.
> With an array-based set however, every insertion first requires a search to check for existence of a duplicate.
> Insertion into the end of an array-based set will take up to N steps to search and one step for the actual insertion, which takes N+1 steps in total.
> In the worst case scenario, inserting a value at the beginning of an array-based set takes N steps to look up for duplicates,
> and N steps to shift all the data to the right, and one last final step to insert the new value.
> That's total of 2N+1 steps.

> Origin: 1

> References:
---
</details>

## Chapter 2/20

<details>
<summary>how many steps does each of four main operations take for an ordered array?</summary>

> **Read:** Same as array and array-based set.
>
> **Delete:** Same as array and array-based set.
>
> **Insert:** In terms of N, it takes N elements in an ordered array, the insertion takes N+2 steps in total, no matter where in the ordered array the new value ends up.
> If the value ends up toward the beginning of the ordered array, we have fewer comparisons and more shifts.
> If the value ends up toward the end, we get more comparisons but fewer shifts.
>
> **Search:** Searching can be applied to ordered array using different algorithms.
> Using **linear search** algorithm, the operation can be stopped early when the value is found.
> Using **binary search** algorithm, it would take only one more step to search each time data set grows twice.

> Origin: 1

> References:
---
</details>

<details>
<summary>What are the advantages and disadvantages of using ordered array over unordered array?</summary>

> Within an ordered array, we can stop a **linear search** early even if the value isn't contained within the array as soon as we reach a value with a higher order.
>
> *pseudocode for C++*
> ```cpp
> iterator linear_search(array, search_value)
> {
>     for (iter = array.begin(); iter != array.end() && *iter >= search_value; ++*iter)
>         if (*iter == search_value)
>             return iter;
>     return nullptr;
> }
> ``````
>
> Though, using an ordered array we can also use **binary search** to attempt a search much faster!
>
> ```cpp
> iterator binary_search(array, search_value)
> {
>     lower_bound = array.begin();
>     upper_bound = array.end() - 1;
> 
>     while (lower_bound <= upper_bound)
>     {
>         midpoint = (upper_bound + lower_bound) / 2;
>         midpoint_value = array[midpoint];
> 
>         if (search_value == midpoint_value)
>             return midpoiont;
>         else if (search_value > midpoint_value)
>             upper_bound = midpoint - 1;
>         else if (search_value < midpoint_value)
>             lower_bound = midpoint + 1;
>     }
> 
>     return nullptr;
> }
> ``````

> Origin: 2

> References:
---
</details>

## Chapter 3/20

<details>
<summary>What is the key question of Big O notation and what does it express?</summary>

> The key question of Big O notation is if there are N data elements, how may steps will the algorithm take?
> Big O tells the story of how much the number of step increases as the data changes.

> Origin: 3

> References:
---
</details>

<details>
<summary>What are the order of main operations for arrays?</summary>

> **Read:** it would take 1 step to read from an array, so it is `O(1)`.
> **Search:** it would take N steps to search through an array, so it is `O(N)`.
> **Insert:** it would take N+1 steps to insert into an array in the worst case scenario, so it is `O(N)`.
> **Delete:** it would take N steps to delete from an array, so it is `O(N)`.

> Origin: 3

> References:
---
</details>

<details>
<summary>What are the order of main operations for array-based sets?</summary>

> **Read:** same as arrays, it would take 1 step to read from an array-based set, so it is `O(1)`.
> **Search:** same as arrays it would take N steps to search through an array-based set, so it is `O(N)`.
> **Insert:** it would take N steps to search first and N steps to insert into an array in the worst case scenario, so it is `O(N)`.
> **Delete:** same as arrays it would take N steps to delete from an array-based set, so it is `O(N)`.

> Origin: 3

> References:
---
</details>

<details>
<summary>What are the order of main operations for ordered arrays?</summary>

> **Read:** same as arrays, it would take 1 step to read from an array-based set, so it is `O(1)`.
> **Search:** same as arrays it would take N steps to search through an array-based set, so it is `O(N)`.
> **Insert:** it would take N steps to search first and N steps to insert into an array in the worst case scenario, so it is `O(N)`.
> **Delete:** same as arrays it would take N steps to delete from an array-based set, so it is `O(N)`.

> Origin: 3

> References:
---
</details>

<details>
<summary>What does the constant time in <code>O(1)</code> mean?</summary>

> In constant time efficiency no matter how many elements exist, the operation always takes one step.

> Origin: 3

> References:
---
</details>

<details>
<summary>What does <code>O(log N)</code> mean in terms of number of steps taken in an algorithm?</summary>

> `O(log N)` means the algorithm takes as many steps as it takes to keep halving the data elements until we remain with 1.

> Origin: 3

> References:
---
</details>

## Chapter 4/20

<details>
<summary>What are the steps taken to sort an array using <b>Bubble Sort</b> algorithm, and what is its efficiency in terms of Big O?</summary>

> 1. Point to two first consecutive values in the array.
> 2. If the two items are out of order, swap them.
> 3. Move pointers one cell to the right.
> 4. Repeat steps 1 through 3 until we reach the end of the array.
> 5. Move back the two pointers back to the first two values of the array, and execute another pass-through of the array until we have a pass-through in which we did not perform any swaps.
>
> Initial array:
>  4  2  7  1  3
>
> First pass-through: 3 swaps
> (4  2) 7  1  3
> (2  4) 7  1  3    swap
>  2 (4  7) 1  3
>  2  4 (7  1) 3
>  2  4 (1  7) 3    swap
>  2  4  1 (7  3)
>  2  4  1 (3  7)   swap
>
> Second pass-through: 2 swaps
> (2  4) 1  3 |7|
>  2 (4  1) 3 |7|
>  2 (1  4) 3 |7|   swap
>  2  1 (4  3)|7|
>  2  1 (3  4)|7|   swap
>
> Third pass-through: 1 swaps
> (2  1) 3 |4  7|
> (1  2) 3 |4  7|   swap
>  1 (2  3)|4  7|
>
> Fourth pass-through:
> (1  2)|3  4  7|   no swap means end of pass-throughs
>
> Sorted array:
>  1  2  3  4  7
> 
> For N elements we make `(N-1) + (N-2) + ... + 1` comparisons and it worst case scenario we make swap for each comparison,
> making it `N²/2` steps which falls into the `O(N²)` general category.

> Origin: 4

> References:
---
</details>

## Chapter 5/20

<details>
<summary>What steps are taken in sorting an array using <b>Selection Sort</b> algorithm and what is its efficiency in terms of Big O?</summary>

> 1. Step through array from left to right to determine which value has least order and keep track of the lowest value we've encountered so far.
> 2. Once we've determined which index contains the lowest value, swap its value with the first value in pass-through.
> 3. Repeat each pass-through from step 1 and 2 until a pass-through starting at the end of the array is reached.
>
> Initial array:
>  4  2  7  1  3
>
> First pass-through: starting at index 0
> (4) 2  7  1  3        lowest value: 4
>  4 (2) 7  1  3        lowest value: 2
>  4  2 (7) 1  3        lowest value: 2
>  4  2  7 (1) 3        lowest value: 1
>  4  2  7  1 (3)       lowest value: 1
> |1| 2  7  4  3        swap lowest value with the first value in pass-through
>
> Second pass-through: starting at index 1
> |1|(2) 7  4  3        lowest value: 2
> |1| 2 (7) 4  3        lowest value: 2
> |1| 2  7 (4) 3        lowest value: 2
> |1| 2  7  4 (3)       lowest value: 2
> |1  2| 7  4  3        no swap needed as lowest value is already the first value in pass-through
>
> Third pass-through: starting at index 2
> |1  2|(7) 4  3        lowest value: 7
> |1  2| 7 (4) 3        lowest value: 4
> |1  2| 7  4 (3)       lowest value: 3
> |1  2  3| 4  7        swap lowest value with the first value in pass-through
>
> Fourth pass-through: starting at index 3
> |1  2  3|(4) 7        lowest value: 4
> |1  2  3| 4 (7)       lowest value: 4
> |1  2  3  4| 7        no swap needed as lowest value is already the first value in pass-through
>
> Fifth pass-through: starting at index 4
> |1  2  3  4|(7)       pass-through starts at the end of the array
>
> Sorted array:
>  1  2  3  4  7

> Origin: 5

> References:
---
</details>

<details>
<summary>What are the general categories of algorithm speeds?</summary>

> * `O(1)`
> * `O(log N)`
> * `O(N)`
> * `O(N²)`
> * `O(N³)`
> * `O(2ⁿ)`

> Origin: 5

> References:
---
</details>

<details>
<summary>How do algorithms are classified into different categories of Big O?</summary>

> It's enough to identify them by their general category.

> Origin: 5

> References:
---
</details>

<details>
<summary>How two algorithms in the same category can be compared?</summary>

> When two algorithms fall under the same classification of Big O, it doesn't necessarily mean that both algorithms have the same speed.
> Further analysis is required to determine which algorithm is faster.

> Origin: 5

> References:
---
</details>

## Chapter 6/20

<details>
<summary>What steps are taken to sort an array using <b>Insertion Sort</b> algorithm and what is its efficiency?</summary>

> 1. In the first pass-through, remove the value at index 1 and store it in a temporary variable. This will leave a gap at that index.
> 2. Begin shifting by taking each value to the left of the gap and compare it to the value in the temporary variable. If the value to the left of the gap is greater from the temporary variable, we shift that value to the right. As soon as a value with lower order than the temporary value encountered in the array, shifting is over.
> 3. Insert the temporarily removed value into the gap.
> 4. Repeat all steps from 1 to 3 until the pass-through begins at the final index of the array.
>
> Initial array:
>
> First pass-through:
>
> Second pass-through:
>
> Third pass-through:
>
> Sorted array:

> Origin: 6

> References:
---
</details>

<details>
<summary>Having different orders of magnitude in the efficiency of an algorithm, how does Big O notation represents the efficiency?</summary>

> Big O notation only takes into account the highest order of N when we have multiple orders added together.
> `N³+2N²+N+5` is expressed as `O(N³)`.

> Origin: 6

> References:
---
</details>

<details>
<summary>What are the wrost, average and best case scenarios of <b>Bubble Sort</b>, <b>Selection Sort</b>, and <b>Insertion Sort</b>?</summary>

> **Bubble Sort:** worst case `O(N²)`, average case `O(N²)`, best case `O(N²)`.
> **Selection Sort:** worst case `O(N²/2)`, average case `O(N²/2)`, best case `O(N²/2)`.
> **Insertion Sort:** worst case `O(N²)`, average case `O(N²/2)`, best case `O(N)`.

> Origin: 6

> References:
---
</details>

<details>
<summary>What considerations should be taken into account when choosing between <b>Insertion Sort</b> and <b>Selection Sort</b>?</summary>

> In an average case, when an array is randomly sorted, they perform similarly.
> If an array can be assumed to be mostly sorted, then **Insertion Sort** will be a better choice.
> If an array is known to be mostly sorted in reverse order, then **Selection Sort** will be faster.

> Origin: 6

> References:
---
</details>

## Chapter 7/20

Nothing to import.

## Chapter 8/20

<details>
<summary>What is a hash table?</summary>

> A hash table is a list of paired values.
> The first item in each pair is called the key, and the second item is called the value.

> Origin: 8

> References:
---
</details>

<details>
<summary>What characteristics should a function have in order to be a valid hash function?</summary>

> A hash function needs to meet only one criterion to be valid.
> It must convert the same string to the same number every single time it's applied.

> Origin: 8

> References:
---
</details>

<details>
<summary>What is the common value of <b>load factor</b> in implementation of hash tables?</summary>

> The ratio of data to cells ideally is 0.7 which means 7 elements per 10 cells.

> Origin: 8

> References:
---
</details>

<details>
<summary>How hash tables can be used for non-paired objects instead of arrays in order to make look up operations efficient?</summary>

> By storing objects as keys and assign boolean true as the associated value for each object.

> Origin: 8

> References:
---
</details>

## Chapter 9/20

<details>
<summary>What constraints do stacks have?</summary>

> * Data can be inserted only at the end of a stack.
> * Data can be deleted only from the end of a stack.
> * Only the last element of a stack can be read.

> Origin: 9

> References:
---
</details>

<details>
<summary>What operations should stack data structures support?</summary>

> * `push()`
> * `pop()`
> * `top()`

> Origin: 9

> References:
---
</details>

<details>
<summary>What is an <b>Abstract Data Type</b>?</summary>

> It's a kind of data structure that is a set of theoretical rules that revolve around some other basic data structures.
> The set, stack, and queue are examples of abstract data types.
> Some implementations of sets use arrays under the hood while other implementations actually use hash tables.
> The set itself, though, is simply a theoertical concept, it's a list of non-duplicated data elements.

> Origin: 9

> References:
---
</details>

<details>
<summary>What constraints do queues have?</summary>

> * The first data added to a queue is the first item to be removed.
> * Data can be inserted only at the end of a queue, similar to stacks.
> * Data can be deleted only from the front of a queue, in opposite behavior of the stacks.
> * Only the element at the front of a queue can be read.

> Origin: 9

> References:
---
</details>

<details>
<summary>What are the main operations of queues?</summary>

> * `enqueue()`
> * `dequeue()`
> * `front()`

> Origin: 9

> References:
---
</details>

## Chapter 10/20

<details>
<summary>What is base case and why all recursive calls should have one?</summary>

> In recursion terminology, the case in which a function will not recurse is knokwn as the base case.
> All recursive functions should have at least one base case. They will keep recalling themselves infinitely otherwise.

> Origin: 10

> References:
---
</details>

<details>
<summary>How recursive code can be read?</summary>

> 1. Identify the base case.
> 2. Walk through the function for the base case.
> 3. Identify the next-to-last case.
> 4. Walk through function for the next-to-last case.
> 5. Repeat this process by identifying before the case you just analyzed, and walking through the function for that case.
>
> ```ruby
> def factorial(number)
>     if number == 1
>         return 1
>     else
>         return number * factorial(number-1)
>     end
> end
> ``````
>
> Writing upwards from base case:
>
> factorial(1) returns 1
>
> And the for the next-to-last case:
>
> factorial(2) returns 2
> factorial(1) returns 1
>
> And again a case before that:
>
> factorial(3) returns 6
> factorial(2) returns 2
> factorial(1) returns 1

> Origin: 10

> References:
---
</details>

<details>
<summary>When does stack overflow occur in recursive calls?</summary>

> In case of infinite recursion, the same function keeps being pushed onto the call stack.
> The call stack will eventually be consumed until further calls will not be possible.

> Origin: 10

> References:
---
</details>

## Chapter 11/20

Nothing to import.

## Chapter 12/20

<details>
<summary>What is <b>Overlapping Subproblems</b> in recursion?</summary>

> When a problem is solved by solving smaller version of the same problem, the smaller problem is called a *subproblem*.
> What makes these subproblem overlapping is the fact that each subproblem calls many of the same functions as each other.

> Origin: 12

> References:
---
</details>

<details>
<summary>What is <b>Dynamic Programming</b> and how can it be an optimization to recursion?</summary>

> **Dynamic Programming** is the process of optimizing recursive problems that have overlapping subproblems.
> Optimizing an algorithm with dynamic programming is typically accomplished with one of two techniques.
> The first technique is something called memoization which reduces recursive calls by remembering previously computed functions.
> The second technique, known as **going bottom-up** uses iteration instead of recursion to prevent duplicate calls.

> Origin: 12

> References:
---
</details>

<details>
<summary>How does <b>Dynamic Programming</b> using <b>Memoization</b> helps in optimizing recursive calls?</summary>

> With memoization, each time we make a new calculation, we store it in a hash table for future calls.
> This way we only make a calculation if it hadn't ever been made before.

> Origin: 12

> References:
---
</details>

<details>
<summary>How does <b>Dynamic Programming</b> using <b>Memoization</b> helps in optimizing recursive calls?</summary>

> By using iteration instead of recursion to ensure that it doesn't make duplicate calls for overlapping subproblems.

> Origin: 12

> References:
---
</details>

## Chapter 13/20
## Chapter 14/20
## Chapter 15/20
## Chapter 16/20
## Chapter 17/20
## Chapter 18/20
## Chapter 19/20
## Chapter 20/20
